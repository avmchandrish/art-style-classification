{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a208be",
   "metadata": {},
   "source": [
    "# Training style classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505b97af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torchsummary\n",
    "# ! pip install --upgrade opencv-python\n",
    "# ! pip install albumentations\n",
    "# ! pip install s3fs\n",
    "# ! pip install tqdm\n",
    "# ! pip install torch\n",
    "# ! pip install torchvision\n",
    "# ! conda install --yes pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge\n",
    "# ! pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d8e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import skimage.io as skio\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b05fae1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>artistName</th>\n",
       "      <th>image</th>\n",
       "      <th>map</th>\n",
       "      <th>paintingUrl</th>\n",
       "      <th>artistUrl</th>\n",
       "      <th>albums</th>\n",
       "      <th>flags</th>\n",
       "      <th>images</th>\n",
       "      <th>style</th>\n",
       "      <th>Image name</th>\n",
       "      <th>Style-image-name</th>\n",
       "      <th>Present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>57728479edc2cb3880fdea33</td>\n",
       "      <td>Silhouette fantastique</td>\n",
       "      <td>1854</td>\n",
       "      <td>500</td>\n",
       "      <td>366</td>\n",
       "      <td>Victor Hugo</td>\n",
       "      <td>https://uploads0.wikiart.org/images/victor-hug...</td>\n",
       "      <td>0*23**67*</td>\n",
       "      <td>/en/victor-hugo/silhouette-fantastique-1854</td>\n",
       "      <td>/en/victor-hugo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abstract-art</td>\n",
       "      <td>silhouette-fantastique-1854.jpg</td>\n",
       "      <td>abstract-art-silhouette-fantastique-1854.jpg</td>\n",
       "      <td>abstract-art-silhouette-fantastique-1854.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>57728a62edc2cb388010efa1</td>\n",
       "      <td>First Communion of Anaemic Young Girls in the ...</td>\n",
       "      <td>1883</td>\n",
       "      <td>1324</td>\n",
       "      <td>848</td>\n",
       "      <td>Alphonse Allais</td>\n",
       "      <td>https://uploads0.wikiart.org/images/alphonse-a...</td>\n",
       "      <td>01234*67*</td>\n",
       "      <td>/en/alphonse-allais/first-communion-of-anaemic...</td>\n",
       "      <td>/en/alphonse-allais</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abstract-art</td>\n",
       "      <td>first-communion-of-anaemic-young-girls-in-the-...</td>\n",
       "      <td>abstract-art-first-communion-of-anaemic-young-...</td>\n",
       "      <td>abstract-art-first-communion-of-anaemic-young-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>57728a61edc2cb388010ef5f</td>\n",
       "      <td>Apoplectic Cardinals Harvesting Tomatoes on th...</td>\n",
       "      <td>1884</td>\n",
       "      <td>1400</td>\n",
       "      <td>980</td>\n",
       "      <td>Alphonse Allais</td>\n",
       "      <td>https://uploads2.wikiart.org/images/alphonse-a...</td>\n",
       "      <td>01234*67*</td>\n",
       "      <td>/en/alphonse-allais/apoplectic-cardinals-harve...</td>\n",
       "      <td>/en/alphonse-allais</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abstract-art</td>\n",
       "      <td>apoplectic-cardinals-harvesting-tomatoes-on-th...</td>\n",
       "      <td>abstract-art-apoplectic-cardinals-harvesting-t...</td>\n",
       "      <td>abstract-art-apoplectic-cardinals-harvesting-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>57728a61edc2cb388010ef71</td>\n",
       "      <td>Band of Greyfriars in the Fog (Band Of Dusty D...</td>\n",
       "      <td>1884</td>\n",
       "      <td>1400</td>\n",
       "      <td>980</td>\n",
       "      <td>Alphonse Allais</td>\n",
       "      <td>https://uploads2.wikiart.org/images/alphonse-a...</td>\n",
       "      <td>01234*67*</td>\n",
       "      <td>/en/alphonse-allais/band-of-greyfriars-in-the-...</td>\n",
       "      <td>/en/alphonse-allais</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abstract-art</td>\n",
       "      <td>band-of-greyfriars-in-the-fog-band-of-dusty-dr...</td>\n",
       "      <td>abstract-art-band-of-greyfriars-in-the-fog-ban...</td>\n",
       "      <td>abstract-art-band-of-greyfriars-in-the-fog-ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>57728a61edc2cb388010ef81</td>\n",
       "      <td>Negroes Fighting in a Tunnel by Night</td>\n",
       "      <td>1884</td>\n",
       "      <td>800</td>\n",
       "      <td>560</td>\n",
       "      <td>Alphonse Allais</td>\n",
       "      <td>https://uploads5.wikiart.org/images/alphonse-a...</td>\n",
       "      <td>0123**67*</td>\n",
       "      <td>/en/alphonse-allais/negroes-fighting-in-a-tunn...</td>\n",
       "      <td>/en/alphonse-allais</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abstract-art</td>\n",
       "      <td>negroes-fighting-in-a-tunnel-by-night.jpg</td>\n",
       "      <td>abstract-art-negroes-fighting-in-a-tunnel-by-n...</td>\n",
       "      <td>abstract-art-negroes-fighting-in-a-tunnel-by-n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                        id  \\\n",
       "0           0  57728479edc2cb3880fdea33   \n",
       "1           1  57728a62edc2cb388010efa1   \n",
       "2           2  57728a61edc2cb388010ef5f   \n",
       "3           3  57728a61edc2cb388010ef71   \n",
       "4           4  57728a61edc2cb388010ef81   \n",
       "\n",
       "                                               title  year  width  height  \\\n",
       "0                             Silhouette fantastique  1854    500     366   \n",
       "1  First Communion of Anaemic Young Girls in the ...  1883   1324     848   \n",
       "2  Apoplectic Cardinals Harvesting Tomatoes on th...  1884   1400     980   \n",
       "3  Band of Greyfriars in the Fog (Band Of Dusty D...  1884   1400     980   \n",
       "4              Negroes Fighting in a Tunnel by Night  1884    800     560   \n",
       "\n",
       "        artistName                                              image  \\\n",
       "0      Victor Hugo  https://uploads0.wikiart.org/images/victor-hug...   \n",
       "1  Alphonse Allais  https://uploads0.wikiart.org/images/alphonse-a...   \n",
       "2  Alphonse Allais  https://uploads2.wikiart.org/images/alphonse-a...   \n",
       "3  Alphonse Allais  https://uploads2.wikiart.org/images/alphonse-a...   \n",
       "4  Alphonse Allais  https://uploads5.wikiart.org/images/alphonse-a...   \n",
       "\n",
       "         map                                        paintingUrl  \\\n",
       "0  0*23**67*        /en/victor-hugo/silhouette-fantastique-1854   \n",
       "1  01234*67*  /en/alphonse-allais/first-communion-of-anaemic...   \n",
       "2  01234*67*  /en/alphonse-allais/apoplectic-cardinals-harve...   \n",
       "3  01234*67*  /en/alphonse-allais/band-of-greyfriars-in-the-...   \n",
       "4  0123**67*  /en/alphonse-allais/negroes-fighting-in-a-tunn...   \n",
       "\n",
       "             artistUrl  albums  flags images         style  \\\n",
       "0      /en/victor-hugo     NaN      2    NaN  abstract-art   \n",
       "1  /en/alphonse-allais     NaN      2    NaN  abstract-art   \n",
       "2  /en/alphonse-allais     NaN      2    NaN  abstract-art   \n",
       "3  /en/alphonse-allais     NaN      2    NaN  abstract-art   \n",
       "4  /en/alphonse-allais     NaN      2    NaN  abstract-art   \n",
       "\n",
       "                                          Image name  \\\n",
       "0                    silhouette-fantastique-1854.jpg   \n",
       "1  first-communion-of-anaemic-young-girls-in-the-...   \n",
       "2  apoplectic-cardinals-harvesting-tomatoes-on-th...   \n",
       "3  band-of-greyfriars-in-the-fog-band-of-dusty-dr...   \n",
       "4          negroes-fighting-in-a-tunnel-by-night.jpg   \n",
       "\n",
       "                                    Style-image-name  \\\n",
       "0       abstract-art-silhouette-fantastique-1854.jpg   \n",
       "1  abstract-art-first-communion-of-anaemic-young-...   \n",
       "2  abstract-art-apoplectic-cardinals-harvesting-t...   \n",
       "3  abstract-art-band-of-greyfriars-in-the-fog-ban...   \n",
       "4  abstract-art-negroes-fighting-in-a-tunnel-by-n...   \n",
       "\n",
       "                                             Present  \n",
       "0       abstract-art-silhouette-fantastique-1854.jpg  \n",
       "1  abstract-art-first-communion-of-anaemic-young-...  \n",
       "2  abstract-art-apoplectic-cardinals-harvesting-t...  \n",
       "3  abstract-art-band-of-greyfriars-in-the-fog-ban...  \n",
       "4  abstract-art-negroes-fighting-in-a-tunnel-by-n...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read artworks df\n",
    "artworks = pd.read_csv('artworks.csv')\n",
    "artworks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24b91a7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abstract-art/silhouette-fantastique-1854.jpg'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create col w/ S3 filename\n",
    "artworks['s3_path'] = artworks.apply(lambda row: row['style'] \\\n",
    "                                     + \"/\" + row['image'].split('/')[-1].split('.')[0] + \".jpg\", \n",
    "                                     axis=1)\n",
    "artworks['s3_path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f36071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>artistName</th>\n",
       "      <th>image</th>\n",
       "      <th>map</th>\n",
       "      <th>paintingUrl</th>\n",
       "      <th>artistUrl</th>\n",
       "      <th>albums</th>\n",
       "      <th>flags</th>\n",
       "      <th>images</th>\n",
       "      <th>style</th>\n",
       "      <th>Image name</th>\n",
       "      <th>Style-image-name</th>\n",
       "      <th>Present</th>\n",
       "      <th>s3_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>57728479edc2cb3880fdea33</td>\n",
       "      <td>Silhouette fantastique</td>\n",
       "      <td>1854</td>\n",
       "      <td>500</td>\n",
       "      <td>366</td>\n",
       "      <td>Victor Hugo</td>\n",
       "      <td>https://uploads0.wikiart.org/images/victor-hug...</td>\n",
       "      <td>0*23**67*</td>\n",
       "      <td>/en/victor-hugo/silhouette-fantastique-1854</td>\n",
       "      <td>/en/victor-hugo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abstract-art</td>\n",
       "      <td>silhouette-fantastique-1854.jpg</td>\n",
       "      <td>abstract-art-silhouette-fantastique-1854.jpg</td>\n",
       "      <td>abstract-art-silhouette-fantastique-1854.jpg</td>\n",
       "      <td>abstract-art/silhouette-fantastique-1854.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>57728a62edc2cb388010efa1</td>\n",
       "      <td>First Communion of Anaemic Young Girls in the ...</td>\n",
       "      <td>1883</td>\n",
       "      <td>1324</td>\n",
       "      <td>848</td>\n",
       "      <td>Alphonse Allais</td>\n",
       "      <td>https://uploads0.wikiart.org/images/alphonse-a...</td>\n",
       "      <td>01234*67*</td>\n",
       "      <td>/en/alphonse-allais/first-communion-of-anaemic...</td>\n",
       "      <td>/en/alphonse-allais</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abstract-art</td>\n",
       "      <td>first-communion-of-anaemic-young-girls-in-the-...</td>\n",
       "      <td>abstract-art-first-communion-of-anaemic-young-...</td>\n",
       "      <td>abstract-art-first-communion-of-anaemic-young-...</td>\n",
       "      <td>abstract-art/first-communion-of-anaemic-young-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                        id  \\\n",
       "0           0  57728479edc2cb3880fdea33   \n",
       "1           1  57728a62edc2cb388010efa1   \n",
       "\n",
       "                                               title  year  width  height  \\\n",
       "0                             Silhouette fantastique  1854    500     366   \n",
       "1  First Communion of Anaemic Young Girls in the ...  1883   1324     848   \n",
       "\n",
       "        artistName                                              image  \\\n",
       "0      Victor Hugo  https://uploads0.wikiart.org/images/victor-hug...   \n",
       "1  Alphonse Allais  https://uploads0.wikiart.org/images/alphonse-a...   \n",
       "\n",
       "         map                                        paintingUrl  \\\n",
       "0  0*23**67*        /en/victor-hugo/silhouette-fantastique-1854   \n",
       "1  01234*67*  /en/alphonse-allais/first-communion-of-anaemic...   \n",
       "\n",
       "             artistUrl  albums  flags images         style  \\\n",
       "0      /en/victor-hugo     NaN      2    NaN  abstract-art   \n",
       "1  /en/alphonse-allais     NaN      2    NaN  abstract-art   \n",
       "\n",
       "                                          Image name  \\\n",
       "0                    silhouette-fantastique-1854.jpg   \n",
       "1  first-communion-of-anaemic-young-girls-in-the-...   \n",
       "\n",
       "                                    Style-image-name  \\\n",
       "0       abstract-art-silhouette-fantastique-1854.jpg   \n",
       "1  abstract-art-first-communion-of-anaemic-young-...   \n",
       "\n",
       "                                             Present  \\\n",
       "0       abstract-art-silhouette-fantastique-1854.jpg   \n",
       "1  abstract-art-first-communion-of-anaemic-young-...   \n",
       "\n",
       "                                             s3_path  \n",
       "0       abstract-art/silhouette-fantastique-1854.jpg  \n",
       "1  abstract-art/first-communion-of-anaemic-young-...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artworks.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac8fae6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "surrealism                    3581\n",
       "symbolism                     3580\n",
       "impressionism                 3580\n",
       "romanticism                   3572\n",
       "art-nouveau-modern            3567\n",
       "realism                       3566\n",
       "abstract-expressionism        3564\n",
       "baroque                       3560\n",
       "naive-art-primitivism         3559\n",
       "rococo                        3557\n",
       "post-impressionism            3532\n",
       "neoclassicism                 3528\n",
       "expressionism                 3497\n",
       "cubism                        3409\n",
       "northern-renaissance          3256\n",
       "pop-art                       2656\n",
       "mannerism-late-renaissance    2512\n",
       "minimalism                    2231\n",
       "abstract-art                  2037\n",
       "early-renaissance             1864\n",
       "ukiyo-e                       1854\n",
       "high-renaissance              1747\n",
       "magic-realism                 1745\n",
       "art-informel                  1629\n",
       "color-field-painting          1591\n",
       "Name: style, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artworks['style'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67e2c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file = 'abstract-art/0-42-69-1969.jpg'\n",
    "img = cv2.imread(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40715242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(433, 640, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68f099",
   "metadata": {},
   "source": [
    "### Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "174441e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transforms = A.Compose([\n",
    "    A.Flip(p=0.5),\n",
    "    A.Rotate(limit=10, \n",
    "             border_mode=cv2.BORDER_CONSTANT, \n",
    "             value=0.0, p=0.75),\n",
    "    A.RandomResizedCrop(width=224, height=224, scale=(0.5, 1), p=1),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225), \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db62897",
   "metadata": {},
   "source": [
    "Normalizes, resulting image looks strange:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09a19dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2400db95e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformed = transforms(image=img.astype(np.uint8))\n",
    "img = transformed['image']\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39465343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac3efe",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d833b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtDataset(Dataset):\n",
    "    def __init__(self, df, label_dict, transforms, fs= None):\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "        self.label_dict = label_dict\n",
    "        self.fs = fs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Get filename and label\n",
    "        filename = row['s3_path']\n",
    "        #label = torch.zeros(25, dtype = torch.long)\n",
    "        label = torch.tensor(label_dict[row['style']], dtype = torch.long)\n",
    "        # Read image, correct color channels\n",
    "        img = self.load_img(filename)\n",
    "#        print(img)\n",
    "        # adding this portion if the image has 4 channels or more -- Chandrish\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.expand_dims(img, axis = 2)\n",
    "            img = np.repeat(img, 3, axis = 2)\n",
    "        elif img.shape[2] > 3:\n",
    "            img = img[:, :, :3]\n",
    "        # Augmentations + normalization\n",
    "        transformed = self.transforms(image=img.astype(np.uint8))\n",
    "        img = transformed['image']\n",
    "        \n",
    "        img = img.transpose(2, 0, 1)\n",
    "        # Convert to tensor\n",
    "        img = torch.tensor(img).float()\n",
    "        #img = torch.permute(2, 0, 1)\n",
    "        return img, label\n",
    "    \n",
    "    def load_img(self, s3_path):\n",
    "        try:\n",
    "            img_arr = skio.imread(s3_path)\n",
    "            img_arr.shape\n",
    "        except:\n",
    "            img_arr = skio.imread('symbolism/baroness-fernand-van-der-bruggen-1900.jpg')\n",
    "        return img_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18457ea7",
   "metadata": {},
   "source": [
    "Try removing bad files from the dataframe\n",
    "8312, 25221, 26899, 29243, 32929, 47916"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df62007",
   "metadata": {},
   "source": [
    "## Transfer learning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2928f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_classification_layer(model, model_type='vgg', num_classes=25):\n",
    "    if model_type == 'vgg':\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
    "        )\n",
    "    elif model_type == 'resnet':\n",
    "        model.fc = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
    "    elif model_type == 'vit':\n",
    "        model.heads = nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "    elif model_type == 'convnext':\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.LayerNorm2d((768,), eps=1e-06, elementwise_affine=True),\n",
    "            nn.Flatten(start_dim=1, end_dim=-1),\n",
    "            nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "        )\n",
    "    else:\n",
    "        print(f'Unknown model_type {model_type}. Acceptable types are: \"vgg\", \"resnet\", \"vit\", or \"convnext\"')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e061e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model, **classargs):\n",
    "    '''\n",
    "    Given an existing model, freeze pre-trained weights and\n",
    "    re-instantiate the classifier.\n",
    "    '''\n",
    "    # Freeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Re-instantiate the classifier head\n",
    "    model = set_classification_layer(model, **classargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0c882d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight gradient is set to True\n",
      "features.0.bias gradient is set to True\n",
      "features.2.weight gradient is set to True\n",
      "features.2.bias gradient is set to True\n",
      "features.5.weight gradient is set to True\n",
      "features.5.bias gradient is set to True\n",
      "features.7.weight gradient is set to True\n",
      "features.7.bias gradient is set to True\n",
      "features.10.weight gradient is set to True\n",
      "features.10.bias gradient is set to True\n",
      "features.12.weight gradient is set to True\n",
      "features.12.bias gradient is set to True\n",
      "features.14.weight gradient is set to True\n",
      "features.14.bias gradient is set to True\n",
      "features.16.weight gradient is set to True\n",
      "features.16.bias gradient is set to True\n",
      "features.19.weight gradient is set to True\n",
      "features.19.bias gradient is set to True\n",
      "features.21.weight gradient is set to True\n",
      "features.21.bias gradient is set to True\n",
      "features.23.weight gradient is set to True\n",
      "features.23.bias gradient is set to True\n",
      "features.25.weight gradient is set to True\n",
      "features.25.bias gradient is set to True\n",
      "features.28.weight gradient is set to True\n",
      "features.28.bias gradient is set to True\n",
      "features.30.weight gradient is set to True\n",
      "features.30.bias gradient is set to True\n",
      "features.32.weight gradient is set to True\n",
      "features.32.bias gradient is set to True\n",
      "features.34.weight gradient is set to True\n",
      "features.34.bias gradient is set to True\n",
      "classifier.0.weight gradient is set to True\n",
      "classifier.0.bias gradient is set to True\n",
      "classifier.3.weight gradient is set to True\n",
      "classifier.3.bias gradient is set to True\n",
      "classifier.6.weight gradient is set to True\n",
      "classifier.6.bias gradient is set to True\n"
     ]
    }
   ],
   "source": [
    "# Load VGG-19\n",
    "vgg = models.vgg19()\n",
    "for name, param in vgg.named_parameters():\n",
    "    print(f\"{name} gradient is set to\", param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "966b56f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Freeze model\n",
    "freeze_model(vgg, num_classes=25, model_type='vgg')\n",
    "#vgg.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eab6cd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight gradient is set to False\n",
      "features.0.bias gradient is set to False\n",
      "features.2.weight gradient is set to False\n",
      "features.2.bias gradient is set to False\n",
      "features.5.weight gradient is set to False\n",
      "features.5.bias gradient is set to False\n",
      "features.7.weight gradient is set to False\n",
      "features.7.bias gradient is set to False\n",
      "features.10.weight gradient is set to False\n",
      "features.10.bias gradient is set to False\n",
      "features.12.weight gradient is set to False\n",
      "features.12.bias gradient is set to False\n",
      "features.14.weight gradient is set to False\n",
      "features.14.bias gradient is set to False\n",
      "features.16.weight gradient is set to False\n",
      "features.16.bias gradient is set to False\n",
      "features.19.weight gradient is set to False\n",
      "features.19.bias gradient is set to False\n",
      "features.21.weight gradient is set to False\n",
      "features.21.bias gradient is set to False\n",
      "features.23.weight gradient is set to False\n",
      "features.23.bias gradient is set to False\n",
      "features.25.weight gradient is set to False\n",
      "features.25.bias gradient is set to False\n",
      "features.28.weight gradient is set to False\n",
      "features.28.bias gradient is set to False\n",
      "features.30.weight gradient is set to False\n",
      "features.30.bias gradient is set to False\n",
      "features.32.weight gradient is set to False\n",
      "features.32.bias gradient is set to False\n",
      "features.34.weight gradient is set to False\n",
      "features.34.bias gradient is set to False\n",
      "classifier.0.weight gradient is set to True\n",
      "classifier.0.bias gradient is set to True\n",
      "classifier.3.weight gradient is set to True\n",
      "classifier.3.bias gradient is set to True\n",
      "classifier.6.weight gradient is set to True\n",
      "classifier.6.bias gradient is set to True\n"
     ]
    }
   ],
   "source": [
    "# Check frozen layers\n",
    "for name, param in vgg.named_parameters():\n",
    "    print(f\"{name} gradient is set to\", param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e645dd4",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03d20f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for easily passing training arguments\n",
    "training_params = {'epochs': 20,\n",
    "                  'batch_size': 16,\n",
    "                  'loss_fct': nn.CrossEntropyLoss()}\n",
    "\n",
    "\n",
    "def eval_model(model, dl, training_params):\n",
    "    # Get GPU if available\n",
    "    device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    # model = model.to(device)\n",
    "    total_loss = 0\n",
    "    total_obs = 0\n",
    "    total_correct = 0\n",
    "    loss_fct = training_params['loss_fct']\n",
    "    for X, y in tqdm(dl):\n",
    "        n_obs = len(y)\n",
    "        # Forward pass and calculate loss\n",
    "        yhat = model(X.to(device))#.softmax(dim=1)\n",
    "        loss = loss_fct(yhat.to(device), y.to(device))\n",
    "        total_loss += n_obs * loss.item()\n",
    "        total_obs += n_obs\n",
    "        # Calculate batch accuracy\n",
    "        ypred = np.argmax(yhat.cpu().detach().numpy(), axis=1)\n",
    "        y_arr = y.detach().numpy()\n",
    "        total_correct += n_obs * accuracy_score(y_arr, ypred)\n",
    "    # Return loss, accuracy\n",
    "    avg_loss = total_loss / total_obs\n",
    "    accuracy = total_correct / total_obs\n",
    "    return avg_loss, accuracy\n",
    "    \n",
    "    \n",
    "def train_model(model, optimizer, scheduler, train_ds, valid_ds, training_params):\n",
    "    # Get loss function\n",
    "    loss_fct = training_params['loss_fct']\n",
    "    # Create dataloaders based on batch size\n",
    "    batch_size = training_params['batch_size']\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    valid_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "    # Get GPU if available\n",
    "    device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    # Train\n",
    "    for _ in range(training_params['epochs']):\n",
    "        # Put model in train mode\n",
    "        model.train()\n",
    "        # Train on training dataloader\n",
    "        for X, y in tqdm(train_dl):\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass and loss calculation\n",
    "            yhat = model(X.to(device))#.softmax(dim=1)\n",
    "            loss = loss_fct(yhat.to(device), y.to(device))\n",
    "            # Backward pass and step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()  # update scheduler each epoch\n",
    "        # Calculate loss, accuracy on train and validation\n",
    "        train_loss, train_acc = eval_model(model, train_dl, training_params)\n",
    "        valid_loss, valid_acc = eval_model(model, valid_dl, training_params)\n",
    "        train_str = f\"train loss: {train_loss:.4f} | train acc: {train_acc:.4f}\"\n",
    "        valid_str = f\" | valid loss: {valid_loss:.4f} | valid acc: {valid_acc:.4f}\"\n",
    "        print(f'[{_}] ' + train_str + valid_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff3c43",
   "metadata": {},
   "source": [
    "### Train Val Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7612a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = artworks.sample(frac = 1, random_state = 62).reset_index(drop = True)\n",
    "split1 = int(0.7 * df.shape[0])\n",
    "split2 = int(0.85 * df.shape[0])\n",
    "train_df, valid_df, test_df = df.iloc[:split1].copy(), df.iloc[split1: split2].reset_index(drop = True), \\\n",
    "                                    df.iloc[split2:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a04c6",
   "metadata": {},
   "source": [
    "### Instantiate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe41abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {style: i for i, style in enumerate(sorted(artworks['style'].unique()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c37e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Datasets\n",
    "train_ds = ArtDataset(train_df, label_dict, transforms)\n",
    "valid_ds = ArtDataset(train_df, label_dict, transforms)\n",
    "test_ds = ArtDataset(train_df, label_dict, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c25cc2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), torch.Size([]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing datasets\n",
    "train_ds[0][0].shape, train_ds[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "474c7508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 224, 224]), torch.Size([128]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing dataloaders\n",
    "x, y = next(iter(train_dl))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0de33",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f722af5",
   "metadata": {},
   "source": [
    "### Training - VGG 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a341c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying VGG 19\n",
    "from torchvision.models import vgg19\n",
    "model = vgg19(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48065464",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# freezing the parameters\n",
    "freeze_model(model, num_classes=25, model_type='vgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e785f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "optimizer = optim.Adam(model.parameters(), )\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.5)\n",
    "training_params = {'epochs': 10,\n",
    "                  'batch_size': 128,\n",
    "                  'loss_fct': nn.CrossEntropyLoss()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e33c057a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                         | 0/398 [00:00<?, ?it/s]/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:819: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  1%|                                                                                                | 2/398 [00:17<57:40,  8.74s/it]/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:625: UserWarning: Metadata Warning, tag 33723 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:819: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  3%|                                                                                             | 12/398 [01:25<44:27,  6.91s/it]/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:625: UserWarning: Metadata Warning, tag 33723 had too many entries: 48, expected 1\n",
      "  warnings.warn(\n",
      "/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:819: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      " 26%|                                                                      | 105/398 [11:35<33:51,  6.93s/it]/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:625: UserWarning: Metadata Warning, tag 33723 had too many entries: 14, expected 1\n",
      "  warnings.warn(\n",
      " 35%|                                                             | 139/398 [15:25<28:26,  6.59s/it]/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:625: UserWarning: Metadata Warning, tag 33723 had too many entries: 26, expected 1\n",
      "  warnings.warn(\n",
      " 38%|                                                          | 152/398 [16:46<24:23,  5.95s/it]/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:625: UserWarning: Metadata Warning, tag 33723 had too many entries: 12, expected 1\n",
      "  warnings.warn(\n",
      " 70%|                            | 280/398 [30:52<13:08,  6.68s/it]/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/PIL/Image.py:2918: DecompressionBombWarning: Image size (107327830 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      " 85%|              | 337/398 [37:06<06:21,  6.26s/it]/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:625: UserWarning: Metadata Warning, tag 33723 had too many entries: 17, expected 1\n",
      "  warnings.warn(\n",
      "100%|| 398/398 [43:38<00:00,  6.58s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "eval_model() missing 1 required positional argument: 'training_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2066364/4141321183.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2066364/2604138506.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, train_ds, valid_ds, training_params)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# update scheduler each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Calculate loss, accuracy on train and validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtrain_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"train loss: {train_loss:.4f} | train acc: {train_acc:.4f}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: eval_model() missing 1 required positional argument: 'training_params'"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, scheduler, train_ds, valid_ds, training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5239a6",
   "metadata": {},
   "source": [
    "### Training - ResNet 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a544231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying Resnet 34\n",
    "from torchvision.models import resnet34\n",
    "model = resnet34(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ff368c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_model(model, num_classes=25, model_type='resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8069f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "optimizer = optim.Adam(model.parameters(), lr = 4e-4)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.5)\n",
    "training_params = {'epochs': 10,\n",
    "                  'batch_size': 256,\n",
    "                  'loss_fct': nn.CrossEntropyLoss()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d69c279",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|                                                                                      | 19/199 [05:54<55:55, 18.64s/it]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2153458/4141321183.py\", line 1, in <module>\n",
      "    train_model(model, optimizer, scheduler, train_ds, valid_ds, training_params)\n",
      "  File \"/tmp/ipykernel_2153458/3394249560.py\", line 49, in train_model\n",
      "    for X, y in tqdm(train_dl):\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/tqdm/std.py\", line 1195, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/tmp/ipykernel_2153458/771183522.py\", line 27, in __getitem__\n",
      "    transformed = self.transforms(image=img.astype(np.uint8))\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/albumentations/core/composition.py\", line 210, in __call__\n",
      "    data = t(force_apply=force_apply, **data)\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/albumentations/core/transforms_interface.py\", line 97, in __call__\n",
      "    return self.apply_with_params(params, **kwargs)\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/albumentations/core/transforms_interface.py\", line 112, in apply_with_params\n",
      "    res[key] = target_function(arg, **dict(params, **target_dependencies))\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/albumentations/augmentations/geometric/rotate.py\", line 88, in apply\n",
      "    return F.rotate(img, angle, interpolation, self.border_mode, self.value)\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/albumentations/augmentations/functional.py\", line 70, in wrapped_function\n",
      "    result = func(img, *args, **kwargs)\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/albumentations/augmentations/geometric/functional.py\", line 79, in rotate\n",
      "    return warp_fn(img)\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/albumentations/augmentations/functional.py\", line 189, in __process_fn\n",
      "    img = process_fn(img, **kwargs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/vambati/anaconda3/envs/deep-learning/lib/python3.9/inspect.py\", line 746, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2153458/4141321183.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2153458/3394249560.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, train_ds, valid_ds, training_params)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Train on training dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;31m# Clear gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2153458/771183522.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Augmentations + normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_with_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36mapply_with_params\u001b[0;34m(self, params, force_apply, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mtarget_dependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dependence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtarget_dependencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/albumentations/augmentations/geometric/rotate.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, img, angle, interpolation, **params)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_LINEAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mborder_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/albumentations/augmentations/functional.py\u001b[0m in \u001b[0;36mwrapped_function\u001b[0;34m(img, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/albumentations/augmentations/geometric/functional.py\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(img, angle, interpolation, border_mode, value)\u001b[0m\n\u001b[1;32m     78\u001b[0m     )\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwarp_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/albumentations/augmentations/functional.py\u001b[0m in \u001b[0;36m__process_fn\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2063\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2064\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2066\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2067\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, scheduler, train_ds, valid_ds, training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce116234",
   "metadata": {},
   "source": [
    "### Training - ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "843b0d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16\n",
    "model = vit_b_16(pretrained = True)\n",
    "freeze_model(model, num_classes=25, model_type='vit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7ab10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "optimizer = optim.Adam(model.parameters(), lr = 4e-4)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.5)\n",
    "training_params = {'epochs': 10,\n",
    "                  'batch_size': 256,\n",
    "                  'loss_fct': nn.CrossEntropyLoss()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ea0be3e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|                                                                                     | 18/199 [07:04<1:11:12, 23.60s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2153753/4141321183.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2153753/3394249560.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, train_ds, valid_ds, training_params)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Train on training dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;31m# Clear gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, scheduler, train_ds, valid_ds, training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f00bd0",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f658e4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6faaaec",
   "metadata": {},
   "source": [
    "### Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "393f8d9d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|                                                                                               | 8/398 [00:55<45:05,  6.94s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2066364/3394249560.py\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(model, dl, training_params)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtotal_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_fct'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mn_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Forward pass and calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2066364/3262476354.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Get filename and label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's3_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1566\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3381\u001b[0m             \u001b[0;31m# if we are a copy, mark as such\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3382\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3383\u001b[0;31m             result = self._constructor_sliced(\n\u001b[0m\u001b[1;32m   3384\u001b[0m                 \u001b[0mnew_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3385\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0mmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.data_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure, allow_2d)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;31m# we will try to copy by-definition here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCExtensionArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.9/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36m_try_cast\u001b[0;34m(arr, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_1d_object_array_from_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mensure_wrapped_if_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"U\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_params = {'epochs': 10,\n",
    "                  'batch_size': 128,\n",
    "                  'loss_fct': nn.CrossEntropyLoss()}\n",
    "\n",
    "train_loss, train_acc = eval_model(model, train_dl, training_params)\n",
    "valid_loss, valid_acc = eval_model(model, valid_dl, training_params)\n",
    "train_str = f\"train loss: {train_loss:.4f} | train acc: {train_acc:.4f}\"\n",
    "valid_str = f\" | valid loss: {valid_loss:.4f} | valid acc: {valid_acc:.4f}\"\n",
    "print(f'[{_}] ' + train_str + valid_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "294a53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/vgg19_epoch1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a369bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(47917,len(train_ds))):\n",
    "    train_ds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb49de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8312, 25221, 26899, 29243, 32929, 47916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[8312]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9f1b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds[8312]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a0478",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = train_df.iloc[i]['s3_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d6cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = skio.imread(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(img.shape) == 2:\n",
    "    img = np.expand_dims(img, axis = 2)\n",
    "    img = np.repeat(img, 3, axis = 2)\n",
    "elif img.shape[2] > 3:\n",
    "    img = img[:, :, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b21be",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e78937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(model, optimizer, scheduler, train_ds, valid_ds, training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0332dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check frozen layers\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} gradient is set to\", param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model\n",
    "yhat = model(x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9bc18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = training_params['loss_fct'](yhat.to(device), y.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f569ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "F.cross_entropy(yhat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6432fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b38ba0",
   "metadata": {},
   "source": [
    "training_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac253714",
   "metadata": {},
   "source": [
    "\n",
    "* torchvision model --> VGG, ResNet, ViT\n",
    "* Learning rate scheduler: https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html\n",
    "* Torchvision models: https://pytorch.org/vision/stable/models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b98e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
